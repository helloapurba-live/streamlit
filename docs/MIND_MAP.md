# ğŸ§  Mental Model & Mind Map: Production ML Dashboard

## ğŸ¯ Core Concept Map

```
                    PRODUCTION ML DASHBOARD
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
    FRONTEND            BACKEND              ML/MLOps
        â”‚                   â”‚                   â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚         â”‚         â”‚         â”‚        â”‚         â”‚
Streamlit  UX      FastAPI  Jobs    Models    Tracking
   â”‚         â”‚         â”‚         â”‚        â”‚         â”‚
Multi-   State   Pydantic Async  PyTorch  MLflow
Page   Management Validation Tasks  Skorch  Feature
                                    Optuna   Store
                                    SHAP     Registry
```

---

## ğŸ“‹ Quick Recall Summary

### **The Problem**
Real-time fraud detection requiring:
- <50ms single predictions
- Batch processing (10K+ rows)
- Explainability (SHAP)
- Continuous improvement (hyperparameter tuning)
- Operator-friendly UX (dashboard)

### **The Stack** (Why Each Tool)

| Tool | Purpose | Key Benefit |
|------|---------|-------------|
| **Streamlit** | Frontend dashboard | Zero-config, 30 lines â†’ full UI |
| **FastAPI** | REST API backend | Auto-docs, async, Pydantic validation |
| **PyTorch** | Deep learning | Flexibility, production-ready |
| **Skorch** | PyTorch â†’ sklearn | Pipeline compatibility, GridSearch, Optuna |
| **Optuna** | Hyperparameter tuning | Bayesian optimization (10 trials vs 100+) |
| **MLflow** | Experiment tracking | Git for models, versioning |
| **SHAP** | Explainability | Feature importance, compliance |
| **Prometheus** | Monitoring | Metrics > logs in production |

### **The Architecture** (5 Layers)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: PRESENTATION (Streamlit)                       â”‚
â”‚ - Multi-page navigation (5 pages)                       â”‚
â”‚ - Real-time polling for batch jobs                      â”‚
â”‚ - Interactive charts (Plotly)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: API (FastAPI)                                  â”‚
â”‚ - POST /predict_single â†’ Real-time scoring              â”‚
â”‚ - POST /predict_batch â†’ Background jobs                 â”‚
â”‚ - GET /job_status/{id} â†’ Job polling                    â”‚
â”‚ - Pydantic validation, Prometheus metrics               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: BUSINESS LOGIC                                 â”‚
â”‚ - Feature extraction                                    â”‚
â”‚ - Model inference (cached)                              â”‚
â”‚ - SHAP explanation (cached)                             â”‚
â”‚ - Job orchestration (SQLite/Redis)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 4: ML MODELS                                      â”‚
â”‚ - sklearn RandomForest (baseline)                       â”‚
â”‚ - PyTorch MLP via Skorch (deep learning)                â”‚
â”‚ - Pipeline: StandardScaler â†’ Model                      â”‚
â”‚ - Trained with SMOTE (imbalanced data)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 5: DATA & MLOPS                                   â”‚
â”‚ - Feature store (Parquet, versioned)                    â”‚
â”‚ - Model registry (joblib + metadata)                    â”‚
â”‚ - Experiment tracking (MLflow)                          â”‚
â”‚ - Training data (synthetic generator)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Key Concepts Mind Map

### **1. Skorch: PyTorch â†” sklearn Bridge**

```
PyTorch Model (nn.Module)
        â”‚
        â”‚ Wrap with Skorch
        â–¼
NeuralNetClassifier
        â”‚
        â”œâ”€ .fit(X, y)           âœ… sklearn compatible
        â”œâ”€ .predict(X)          âœ… sklearn compatible
        â”œâ”€ .predict_proba(X)    âœ… sklearn compatible
        â”‚
        â”œâ”€ Works in Pipeline    âœ… StandardScaler â†’ Model
        â”œâ”€ Works in GridSearch  âœ… Hyperparameter tuning
        â””â”€ Works with Optuna    âœ… Bayesian optimization
```

**Mental Model**: Skorch is an adapter pattern. PyTorch speaks "tensors and gradients," sklearn speaks "fit/predict." Skorch translates.

**Critical Bug to Avoid**:
```python
# âŒ WRONG: Don't apply softmax when using CrossEntropyLoss
def forward(self, x):
    return F.softmax(self.fc(x), dim=-1)

# âœ… CORRECT: Return raw logits
def forward(self, x):
    return self.fc(x)  # CrossEntropyLoss applies log-softmax internally
```

---

### **2. FastAPI: Request â†’ Validation â†’ Response**

```
HTTP Request
    â”‚
    â”œâ”€ Pydantic Model validates
    â”‚  (type checking, range validation, auto-docs)
    â”‚
    â”œâ”€ Endpoint function executes
    â”‚  (business logic, model inference)
    â”‚
    â”œâ”€ Prometheus records metrics
    â”‚  (latency, counts, errors)
    â”‚
    â””â”€ JSON Response
       (with proper HTTP status codes)
```

**Mental Model**: Pydantic is a type-safe wall. Invalid requests never reach your code.

**Pattern**:
```python
class Transaction(BaseModel):
    amount: float = Field(gt=0)  # Must be positive

@app.post("/predict")
def predict(txn: Transaction):  # Already validated!
    return model.predict(...)
```

---

### **3. Streamlit: Script Reruns on Interaction**

```
User loads page
    â”‚
    â”œâ”€ Script runs top-to-bottom
    â”‚
User clicks button
    â”‚
    â”œâ”€ Entire script reruns
    â”‚  (not just button handler)
    â”‚
Use st.session_state to persist data
    â”‚
    â””â”€ Values survive across reruns
```

**Mental Model**: Streamlit is like a React component that rerenders on every state change, but for Python.

**Pattern**:
```python
# Save state across reruns
if 'job_id' not in st.session_state:
    st.session_state.job_id = None

if st.button("Submit"):
    st.session_state.job_id = submit_job()
    st.rerun()  # Force immediate rerun
```

---

### **4. Optuna: Bayesian Optimization**

```
Trial 1: lr=0.01, hidden=64  â†’ ROC-AUC=0.85
    â”‚
    â”œâ”€ Optuna learns: "Lower lr might be better"
    â”‚
Trial 2: lr=0.001, hidden=128 â†’ ROC-AUC=0.90
    â”‚
    â”œâ”€ Optuna learns: "lr=0.001 good, try bigger network"
    â”‚
Trial 3: lr=0.001, hidden=256 â†’ ROC-AUC=0.92
    â”‚
    â””â”€ Converges to optimum in 10 trials (vs 100+ for grid search)
```

**Mental Model**: Optuna is GPS navigation. GridSearch is randomly driving.

**Pattern**:
```python
def objective(trial):
    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)  # Log scale!
    model = NeuralNetClassifier(..., lr=lr)
    return cross_val_score(model, X, y, cv=3).mean()

study.optimize(objective, n_trials=10)
```

---

### **5. MLflow: Experiment Tracking**

```
Experiment: "fraud_detection_pytorch"
    â”‚
    â”œâ”€ Run 1 (2025-01-20 10:00)
    â”‚   â”œâ”€ Params: lr=0.001, batch_size=128
    â”‚   â”œâ”€ Metrics: roc_auc=0.89, precision=0.87
    â”‚   â””â”€ Artifacts: model.pkl, plots/
    â”‚
    â”œâ”€ Run 2 (2025-01-20 14:30)
    â”‚   â”œâ”€ Params: lr=0.0005, batch_size=256
    â”‚   â”œâ”€ Metrics: roc_auc=0.91, precision=0.89
    â”‚   â””â”€ Artifacts: model.pkl, plots/
    â”‚
    â””â”€ Compare runs, download best model
```

**Mental Model**: MLflow is Git for models. Commit = Run.

**Pattern**:
```python
with mlflow.start_run():
    mlflow.log_params({"lr": 0.001, "epochs": 50})
    model.fit(X, y)
    mlflow.log_metric("roc_auc", 0.91)
    mlflow.sklearn.log_model(model, "model")
```

---

### **6. SHAP: Model Explanation**

```
Prediction: 78% fraud probability
    â”‚
Why?
    â”‚
    â”œâ”€ amount (+0.25)         â†’ 7x customer baseline
    â”œâ”€ is_night (+0.12)       â†’ Transaction at 11 PM
    â”œâ”€ distance (+0.08)       â†’ 150 miles from home
    â””â”€ merchant_risk (+0.05)  â†’ High-risk merchant category
```

**Mental Model**: SHAP values = "How much does changing this feature change the prediction?"

**Pattern**:
```python
import shap

explainer = shap.TreeExplainer(model)  # For tree models
shap_values = explainer.shap_values(X_test)

# For single prediction
shap_values[1][0]  # Fraud class, first sample
# â†’ [0.25, 0.12, 0.08, ...]  # Feature contributions
```

---

## ğŸ“ Production Patterns

### **Pattern 1: Model Caching**

```python
# âŒ BAD: Load on every request (500ms)
@app.post("/predict")
def predict(txn: Transaction):
    model = joblib.load("model.pkl")  # Disk I/O every time!
    return model.predict(...)

# âœ… GOOD: Load once at startup (5ms)
model = joblib.load("model.pkl")  # Load once globally

@app.post("/predict")
def predict(txn: Transaction):
    return model.predict(...)  # Memory access
```

### **Pattern 2: Background Jobs**

```python
# âŒ BAD: Block API request
@app.post("/predict_batch")
def predict_batch(file: UploadFile):
    df = pd.read_csv(file.file)  # Blocks for 30 seconds!
    predictions = model.predict(df)
    return predictions

# âœ… GOOD: Return immediately, process in background
@app.post("/predict_batch")
async def predict_batch(file: UploadFile, bg: BackgroundTasks):
    job_id = uuid.uuid4()
    bg.add_task(process_batch, job_id, file)
    return {"job_id": job_id, "status": "pending"}
```

### **Pattern 3: Training-Serving Skew**

```python
# Problem: Model trained on customer_avg_amount feature
# But how do we compute this at serving time?

# âŒ BAD: Use dummy value
features = [txn.amount, 100.0, ...]  # Wrong distribution!

# âœ… GOOD: Maintain feature cache
feature_cache = {
    "CUST_001": {"avg_amount": 120.0, "std": 45.0},
    # ... updated daily from transaction history
}

features = [
    txn.amount,
    feature_cache[txn.customer_id]["avg_amount"],
    ...
]
```

---

## ğŸš¨ Common Pitfalls

### **Pitfall 1: PyTorch + CrossEntropyLoss**
âŒ Applying softmax in forward() â†’ destroys gradients
âœ… Return logits, let CrossEntropyLoss handle softmax

### **Pitfall 2: Streamlit State**
âŒ Using global variables â†’ lost on rerun
âœ… Using st.session_state â†’ persists

### **Pitfall 3: Optuna Search Space**
âŒ Linear search for learning rate (0.001, 0.002, ...)
âœ… Log scale search (1e-4, 1e-3, 1e-2) using `log=True`

### **Pitfall 4: FastAPI Blocking**
âŒ Long operations in endpoint â†’ API freezes
âœ… BackgroundTasks for async processing

### **Pitfall 5: Model Versioning**
âŒ Overwriting model.pkl â†’ no rollback
âœ… Version models (model_v1.pkl, model_v2.pkl) + metadata

---

## ğŸ“Š Decision Trees

### **When to Use What?**

```
Need to train model?
    â”‚
    â”œâ”€ Tabular data + interpretability â†’ sklearn RandomForest
    â”œâ”€ Tabular data + performance â†’ XGBoost/LightGBM
    â”œâ”€ Deep learning required â†’ PyTorch + Skorch
    â””â”€ Time series â†’ LSTM/Transformer (PyTorch)

Need to tune hyperparameters?
    â”‚
    â”œâ”€ <5 parameters, discrete â†’ GridSearch
    â”œâ”€ >5 parameters, continuous â†’ Optuna
    â””â”€ Neural networks â†’ Optuna (Bayesian >> Grid)

Need to deploy model?
    â”‚
    â”œâ”€ Simple API â†’ FastAPI
    â”œâ”€ Dashboard â†’ Streamlit
    â”œâ”€ High throughput â†’ Triton Inference Server
    â””â”€ Edge devices â†’ ONNX Runtime

Need to explain predictions?
    â”‚
    â”œâ”€ Tree models â†’ SHAP TreeExplainer (fast)
    â”œâ”€ Linear models â†’ Coefficients
    â”œâ”€ Deep learning â†’ SHAP KernelExplainer (slow)
    â””â”€ Any model â†’ LIME (model-agnostic)
```

---

## ğŸ¯ Mental Shortcuts

### **The 90/10 Rule**
- 10% of effort: Build model with 90% accuracy
- 90% of effort: Deploy at <50ms latency, monitor, retrain, explain

### **The CAP Theorem of ML**
You can optimize for 2 of 3:
- **Speed**: Fast inference (<50ms)
- **Accuracy**: High performance (>95% ROC-AUC)
- **Interpretability**: Explainable decisions (SHAP)

Choose: Fast + Accurate = Deep learning (black box)
       Fast + Interpretable = Logistic regression
       Accurate + Interpretable = RandomForest with SHAP

### **The Validation Hierarchy**
1. **Type validation**: Pydantic (compile-time)
2. **Business logic validation**: Custom validators (runtime)
3. **Model validation**: Cross-validation (training-time)
4. **Production validation**: A/B testing (deploy-time)

---

## ğŸ”„ Workflow Memory Aids

### **Training Workflow**
```
Data â†’ Features â†’ Split â†’ SMOTE â†’ Train â†’ Validate â†’ Log â†’ Save
 â†“       â†“         â†“       â†“       â†“       â†“        â†“     â†“
CSV   Engineer  80/20  Balance  Model    CV     MLflow  .pkl
```

### **Inference Workflow**
```
Request â†’ Validate â†’ Extract â†’ Scale â†’ Predict â†’ Explain â†’ Log â†’ Response
   â†“        â†“          â†“        â†“        â†“         â†“       â†“       â†“
 JSON    Pydantic   Features  Same    Model     SHAP  Prometheus JSON
                             as train
```

### **Deployment Workflow**
```
Code â†’ Test â†’ Docker â†’ Deploy â†’ Monitor â†’ Alert â†’ Rollback
 â†“      â†“       â†“        â†“        â†“        â†“        â†“
Git   pytest  Build   k8s/VM  Prometheus Slack  Previous
                                                  version
```

---

## ğŸ“ Cheat Sheet Formulas

### **Model Performance**
- **Precision** = TP / (TP + FP) â†’ "Of predicted fraud, how many were actual fraud?"
- **Recall** = TP / (TP + FN) â†’ "Of actual fraud, how many did we catch?"
- **F1** = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
- **ROC-AUC** = Area under curve â†’ "Probability model ranks fraud > normal"

### **API Performance**
- **Latency (p99)** = 99th percentile response time
- **Throughput** = Requests per second (RPS)
- **Error Rate** = Failed requests / Total requests
- **Availability** = Uptime / (Uptime + Downtime)

### **Cost-Benefit**
- **False Positive Cost** = Legitimate transaction blocked â†’ customer frustration
- **False Negative Cost** = Fraud not caught â†’ financial loss
- **Threshold** = Optimize based on cost ratio

---

## ğŸ§© Integration Map

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Streamlit  â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP (requests)
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚   FastAPI   â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ joblib.load()
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  sklearn â”‚         â”‚  PyTorch â”‚
    â”‚   Model  â”‚         â”‚ + Skorch â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ MLflow
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚  Experiment â”‚
              â”‚   Tracking  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¬ The Big Picture

**You're building a real-time decision system that:**
1. Scores transactions in <50ms (FastAPI + cached models)
2. Explains decisions to compliance (SHAP)
3. Processes batches asynchronously (BackgroundTasks)
4. Improves continuously (Optuna + MLflow)
5. Serves operators visually (Streamlit)

**Key insight**: This isn't a "model in a Jupyter notebook." It's a **full-stack ML application** with frontend, backend, ML, MLOps, and monitoring.

---

## ğŸš€ Quick Reference Commands

```bash
# Data generation
python data/generate_synthetic_transactions.py

# Training
python src/ml/train_sklearn.py      # RandomForest
python src/ml/train_pytorch.py      # PyTorch + Skorch
python src/ml/optuna_tune.py        # Hyperparameter tuning

# Deployment
uvicorn src.backend.app:app --reload         # Backend
streamlit run src/frontend/app.py            # Frontend
mlflow ui --backend-store-uri sqlite:///mlruns/mlflow.db  # MLflow

# Testing
pytest tests/ -v --cov=src
```

---

## ğŸ’¡ Interview Talking Points

**When asked "How do you deploy ML models?"**
> "I build FastAPI endpoints with Pydantic validation for type safety. Models are loaded once at startup for <50ms latency. For batch processing, I use BackgroundTasks to avoid blocking the API. I monitor with Prometheus metrics (latency, throughput, error rate) and explain predictions with SHAP for compliance."

**When asked "How do you tune hyperparameters?"**
> "For deep learning, I use Optuna's Bayesian optimization instead of GridSearchâ€”it converges in 10 trials vs 100+. Skorch makes PyTorch models sklearn-compatible, so they work seamlessly with Optuna. I track all experiments in MLflow to compare runs and reproduce results."

**When asked "How do you build ML dashboards?"**
> "I use Streamlit for operator UIsâ€”it's 30 lines of Python vs 300 lines of React. The dashboard polls FastAPI endpoints for batch job status, displays SHAP explanations visually, and uses session_state to persist data across reruns. This gives operators a production-ready interface without frontend engineering."

---

**Remember**: Production ML is 10% model building, 90% engineering. Master the 90%.
